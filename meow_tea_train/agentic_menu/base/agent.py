# Copyright 2025 Ruiyi Wang, PEARLS Lab, UC San Diego
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from abc import ABC, abstractmethod
from typing import List, Tuple, Dict, Any
import torch
import numpy as np

from verl import DataProto
from .env import BaseEnv


class BaseAgent(ABC):
    """Base abstract class for all agents in the agentic menu."""
    
    def __init__(
        self,
        env_name: str,
        prompts: DataProto,
        inference_engine: Any,
        sampling_params: Any,
        tokenizer: Any,
        max_iter: int,
        n_traj: int,
        max_prompt_len: int,
        max_response_len: int,
        **kwargs
    ):
        self.env_name = env_name
        self.device = prompts.batch["input_ids"].device
        self.inference_engine = inference_engine
        self.sampling_params = sampling_params
        self.tokenizer = tokenizer
        self.max_iter = max_iter
        self.n_traj = n_traj
        self.max_prompt_len = max_prompt_len
        self.max_response_len = max_response_len
        
        # Token IDs
        self.eos_token_id = tokenizer.eos_token_id
        self.pad_token_id = tokenizer.pad_token_id
        self.sep_token_id = tokenizer.eos_token_id
        self.sep_token = tokenizer.convert_ids_to_tokens(self.sep_token_id)
        
        # Repeat input batch according to n_traj
        self.input_batch = prompts.repeat(repeat_times=self.n_traj, interleave=True)
        self.batch_size = self.input_batch.batch["input_ids"].size(0)
    
    @abstractmethod
    def load_env(self, instance_dir: str, instance_id: str) -> BaseEnv:
        """
        Load environment instance.
        
        Args:
            instance_dir (str): Directory containing environment instances
            instance_id (str): ID of the specific instance
            
        Returns:
            BaseEnv: Environment instance
        """
        pass
    
    @abstractmethod
    def interact(self, instance_env: BaseEnv, action: str) -> Tuple[str, bool, float, BaseEnv]:
        """
        Interact with environment for one step.
        
        Args:
            instance_env (BaseEnv): Environment instance
            action (str): Action to take
            
        Returns:
            Tuple[str, bool, float, BaseEnv]: (next_observation, done, reward, updated_env)
        """
        pass
    
    def run(self) -> DataProto:
        """
        Main function for generating multi-turn rollouts.
        
        Returns:
            DataProto: Rollout results
        """
        pass
    
    @abstractmethod
    def batch_generate(self, messages_batch: List[List[Dict]]) -> Tuple[List[str], List[bool]]:
        """Generate batch of actions using the language model."""
        pass
    
    @abstractmethod
    def convert_result_to_dataproto(self, rollout_state: Dict[str, Any]) -> Tuple[Dict, Dict]:
        """Convert rollout results to DataProto format."""
        pass

    
    # Utility methods that can be shared across implementations
    def _has_sep_token(self, output_ids_batch: List[List[int]]) -> List[bool]:
        """Check if output contains separation token."""
        valid_action_batch_idx = []
        for output_ids in output_ids_batch:
            positions = torch.where(torch.tensor(output_ids) == self.sep_token_id)[0]
            valid_action_batch_idx.append(len(positions) > 0)
        return valid_action_batch_idx

    def _truncate_system_template(self, text: str) -> str:
        import re
        """
        Given string generated by chat template, truncate the system prompt block.
        Example:
            <im_start>system ... <im_end><im_start>user ... <im_end><im_start>assistant
            becomes ->
            <im_start>user ... <im_end><im_start>assistant
        """
        pattern = r'<\|im_start\|>system\n.*?<\|im_end\|>\n'
        return re.sub(pattern, '', text, flags=re.DOTALL)
    
    def _exceeds_prompt_length(self, messages: List[Dict]) -> bool:
        """Check if prompt exceeds maximum length."""
        prompt_tokens = self.tokenizer.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_tensor="pt"
        )
        return len(prompt_tokens) >= self.max_prompt_len
    
    def _create_consistent_object_array(self, data):
        """Create consistent 1D object array."""
        result = np.empty(len(data), dtype=object)
        for i, item in enumerate(data):
            result[i] = item if isinstance(item, list) else [item]
        return result